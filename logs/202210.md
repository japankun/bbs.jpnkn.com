# 2022年10月2日

- MQTT(TLS)のCPU使用率が100%に張り付く怪しい時がある。

<img src="https://t1.jpnkn.com/wp-content/uploads/2022/10/02214403/2022-10-02-21_41_51-Portainer-_-local.png" width="400">

# 2022年10月4日

- テスト用にインスタンスを建てようとしたら以下のエラー

```
CreateInstances[ap-northeast-1]

Sorry, you've reached your maximum limit of Lightsail Instances : 2. If you're new to Lightsail, please try again later.
If the issue persists, please contact Customer Support.

ResourceLimitExceeded
```

- インスタンスの最大数はリージョンごとに20なのでクオータの引き上げをリクエスト
- https://docs.aws.amazon.com/ja_jp/general/latest/gr/lightsail.html#limits_lightsail

# 2022年10月8日

- 16:17 CPUアラート発生
- 16:25 CPUアラート自動解除
- 16:30 対応開始
- 16:35 SSHの反応が既に重く、何らかのリソース制限を疑うもメトリクス上は特に異常な数値ではない。
- 16:45 htopを眺めていても原因らしいプロセスが見当たらず、使用状況も至って普通。
- 16:53 念のためサーバースナップショットを作成
- 16:55 重さが軽減されないので不要なコンテナであるRedisInsightを停止
- 17:20 一時的に軽くなるも、徐々に重くなる。
- 17:30 コンテナごとに絞り、再起動→様子見の連続
- 17:50 一向に改善しないため、AWSのWEBコンソールからサーバー再起動
- 18:00 サーバーが再起動されないので、AWSのWEBコンソールからサーバーを停止
- 18:02 AWSのWEBコンソールからサーバー起動
- 18:05 各種アップデート及び、コンテナごとに復帰
- 18:20 Docker Composeの未使用コンテナがポートを使用したままになり対応
- 18:25 Docker Engine再起動で回復
- 18:30 サーバー全復帰

# 2022年10月9日

- ↑の件の調査続き
- 落ちた近辺の /var/log/messages を見に行ったところ、dockerdが名前解決時にi/o timeoutを吐いているログを発見
- ```Oct  8 16:17:48 dockerd: time="2022-10-08T16:17:11.173522129+09:00" level=warning msg="[resolver] connect failed: dial udp 172.26.0.2:53: i/o timeout"```
- 続いてMackerel-agentが情報を送信できなくなる。
- ```Oct  8 16:17:49 mackerel-agent: 2022/10/08 16:17:03 WARNING <command> Failed to post metrics value (will retry): Post "https://api.mackerelio.com/api/v0/tsdb": context deadline exceeded (Client.Timeout exceeded while awaiting headers)```
- 更にApacheがスワップして固まる
- ```kernel: INFO: task apache2:**** blocked for more than 120 seconds.```
- 原因がVPC DNSのスロット制限ではないかというような情報があり
- https://aws.amazon.com/jp/premiumsupport/knowledge-center/vpc-find-cause-of-failed-dns-queries/
- Amazon が提供する DNS サーバーは、Elastic Network Interface ごとに 1 秒あたり 1024 パケットの制限を適用します。Amazon が提供する DNS サーバーは、この制限を超えたトラフィックをすべて拒否します。
- とあるので、ローカルDNSキャッシュを立ててみることにする。
